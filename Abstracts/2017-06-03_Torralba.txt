Filename: 2017-06-03_Torralba.txt
Author: Torralba, Antonio
Citation Date: 2017/06/03
Abstract URL: https://arxiv.org//abs/1706.00932
Title: See, Hear, and Read: Deep Aligned Representations
Abstract: We capitalize on large amounts of readily-available, synchronous data to
learn a deep discriminative representations shared across three major natural
modalities: vision, sound and language. By leveraging over a year of sound from
video and millions of sentences paired with images, we jointly train a deep
convolutional network for aligned representation learning. Our experiments
suggest that this representation is useful for several tasks, such as
cross-modal retrieval or transferring classifiers between modalities. Moreover,
although our network is only trained with image+text and image+sound pairs, it
can transfer between text and sound as well, a transfer the network never
observed during training. Visualizations of our representation reveal many
hidden units which automatically emerge to detect concepts, independent of the
modality.
