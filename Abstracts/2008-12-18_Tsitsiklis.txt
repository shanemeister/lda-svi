Filename: 2008-12-18_Tsitsiklis.txt
Author: Tsitsiklis, John
Citation Date: 2008/12/18
Abstract URL: https://arxiv.org//abs/0812.3465
Title: Linearly Parameterized Bandits
Abstract: We consider bandit problems involving a large (possibly infinite) collection
of arms, in which the expected reward of each arm is a linear function of an
$r$-dimensional random vector $\mathbf{Z} \in \mathbb{R}^r$, where $r \geq 2$.
The objective is to minimize the cumulative regret and Bayes risk. When the set
of arms corresponds to the unit sphere, we prove that the regret and Bayes risk
is of order $\Theta(r \sqrt{T})$, by establishing a lower bound for an
arbitrary policy, and showing that a matching upper bound is obtained through a
policy that alternates between exploration and exploitation phases. The
phase-based policy is also shown to be effective if the set of arms satisfies a
strong convexity condition. For the case of a general set of arms, we describe
a near-optimal policy whose regret and Bayes risk admit upper bounds of the
form $O(r \sqrt{T} \log^{3/2} T)$.
