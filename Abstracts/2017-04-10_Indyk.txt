Filename: 2017-04-10_Indyk.txt
Author: Indyk, Piotr
Citation Date: 2017/04/10
Abstract URL: https://arxiv.org//abs/1704.02958
Title: On the Fine-Grained Complexity of Empirical Risk Minimization: Kernel  Methods and Neural Networks
Abstract: Empirical risk minimization (ERM) is ubiquitous in machine learning and
underlies most supervised learning methods. While there has been a large body
of work on algorithms for various ERM problems, the exact computational
complexity of ERM is still not understood. We address this issue for multiple
popular ERM problems including kernel SVMs, kernel ridge regression, and
training the final layer of a neural network. In particular, we give
conditional hardness results for these problems based on complexity-theoretic
assumptions such as the Strong Exponential Time Hypothesis. Under these
assumptions, we show that there are no algorithms that solve the aforementioned
ERM problems to high accuracy in sub-quadratic time. We also give similar
hardness results for computing the gradient of the empirical loss, which is the
main computational burden in many non-convex learning tasks.
